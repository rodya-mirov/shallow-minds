{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout is a fundamentally new kind of regularization, if you can even call it that. While \"the exact mechanism [by which it helps] is unclear,\" dropout seems to cause individual neurons to be more intrinsically interesting, rather than just co-adapting with their neighbors.\n",
    "\n",
    "The algorithm itself is easy to describe. **During training**, for each mini-batch (or sometimes, each individual example), we designate a random subset of the inputs to each level and simply \"turn them off\" -- that is, set them to zero.  That means each neuron has to develop without knowing which neurons will actually feed into them.\n",
    "\n",
    "**During testing** (or whenever the neural network is used for actual prediction), we do not drop any inputs off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load_ext autoreload\n",
    "#autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code\n",
    "\n",
    "Recall our forward propagation code looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(weights, biases, activations, input_data):\n",
    "    l = len(weights)\n",
    "    \n",
    "    x = [0] * l # input to level i\n",
    "    z = [0] * l # un-activated output of level i\n",
    "    y = [0] * l # activated output of level i\n",
    "    \n",
    "    x[0] = input_data\n",
    "    \n",
    "    for i in range(0, l):\n",
    "        expanded_bias = np.ones((x[i].shape[0], 1)) * biases[i]\n",
    "        z[i] = np.dot(x[i], weights[i]) + expanded_bias\n",
    "        y[i] = activations[i](z[i])\n",
    "        \n",
    "        if i < l-1:\n",
    "            x[i+1] = y[i]\n",
    "    \n",
    "    return x, z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `x[i]` is the input to level `i`, so `x[0]` is the input to the network, `x[1]` is the input to the first hidden layer, and so on.  We can do all our dropout by modifying `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop_dropout(weights, biases, activations, input_data, drop_rates):\n",
    "    \n",
    "    l = len(weights)\n",
    "    \n",
    "    x = [0] * l # input to level i\n",
    "    z = [0] * l # un-activated output of level i\n",
    "    y = [0] * l # activated output of level i\n",
    "    mask = [0] * l # dropout mask for the input to level i\n",
    "    \n",
    "    # Apply a dropout mask and scaling to x[0]\n",
    "    mask[0] = np.random.random(input_data.shape) > drop_rates[0]\n",
    "    x[0] = input_data * (mask[0]) / (1-drop_rates[0])\n",
    "    \n",
    "    for i in range(0, l):\n",
    "        expanded_bias = np.ones((x[i].shape[0], 1)) * biases[i]\n",
    "        z[i] = np.dot(x[i], weights[i]) + expanded_bias\n",
    "        y[i] = activations[i](z[i])\n",
    "        \n",
    "        if i < l-1:\n",
    "            mask[i+1] = np.random.random(y[i].shape) > drop_rates[i+1]\n",
    "            x[i+1] = y[i] * (mask[i+1]) / (1-drop_rates[i+1])\n",
    "    \n",
    "    return x, z, y, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only had to change a few lines -- `8` to keep track of the masks, `11` and `12` to mask the inputs to the network, and `20` and `21` to mask the inputs to the hidden layers. Let's explain the new code:\n",
    "\n",
    "1. `np.random.random(mat.shape)` gives a random matrix matching the given shape. The numbers are uniformly generated from zero to one.\n",
    "2. `mat > p` gives a matrix of the same shape as `mat`, which is True or False, depending on whether the specific entry of the matrix is less than `p`. Underneath, True is 1 and False is 0, which lets us use it for arithmetic.  Note that this sets an entry of matrix to zero with probability `p`, and sets it to one with probability `1-p`.\n",
    "3. `input_data * (np.random.random(input_data.shape) > dropout_rates[0])` multiplies `input_data` by a matrix of zeros and ones, meaning it sets the failures to zero and leaves the rest untouched.\n",
    "4. `mat / (1-p)` increases the values of `mat` by dividing them by `1-p`.  This compensates exactly for the average amount of total input we're losing because of the dropout.  For example if `p` were `1/3`, then we're losing `1/3` of the inputs, so keeping only `2/3` of it.  To compensate, we scale the remaining inputs by `3/2`, so that on average, the total input is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about our Implementation\n",
    "\n",
    "There are several small notes worth making in the above.\n",
    "\n",
    "First, by using a completely random matrix in step 1, we are dropping out different inputs in each row.  This means that instead of having a fixed dropout choice for the entire mini-batch, we are dropping out different examples independently.  Although the <a href=\"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\">original paper</a> was ambiguous on this point, further research, such as the the introduction on <a href=\"http://www.matthewzeiler.com/pubs/icml2013/icml2013.pdf\">DropConnect</a>, suggests that this gives more regularization and is better.  It's also convenient to code.\n",
    "\n",
    "Second, we compensate for the dropout by scaling as we dropout.  In the original paper, they did not compensate during the training, but did compensate during testing.  Our method is mathematically equivalent, but has the advantage that the network can be tested (or used) without reference to training parameters like the dropout rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About Training?\n",
    "\n",
    "It doesn't seem fair to penalize weights leading to a neuron which was turned off, and in fact we don't.  This is why we needed to keep track of the masks. The original paper declares \"any training example which does not use a parameter will not contribute to the gradient of the parameter.\"  What this means is that if the neuron's output was disabled in a particular row, disable all contribution of that row to the neuron's weights.\n",
    "\n",
    "Since `back_prop` aggregates the total change for each neuron, but we need to decide example-by-example whether to zero out any changes, we'll have to make the changes within `back_prop`.  The original method looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(weights, biases, acts, cost_function,\n",
    "              train_X, train_Y,\n",
    "              x, y, z):\n",
    "    L = len(weights) # number of layers\n",
    "    \n",
    "    cost_diff = cost_function(y[-1], train_Y, diff=True)\n",
    "    \n",
    "    # Gradient of cost at each level\n",
    "    bp_grad = [0] * L\n",
    "\n",
    "    # The last level is special\n",
    "    bp_grad[L-1] = cost_diff * acts[L-1](z[L-1], y[L-1], diff=True)\n",
    "    \n",
    "    # The rest of the levels are just gotten by propagating backward\n",
    "    for i in range(L-2, -1, -1):\n",
    "        scaled_grad = bp_grad[i+1] * acts[i+1](z[i+1], y[i+1], diff=True)\n",
    "        bp_grad[i] = np.dot(scaled_grad, weights[i+1].T)\n",
    "    \n",
    "    # Now adjust for the weights and biases themselves\n",
    "    bp_grad_w = [0] * L\n",
    "    bp_grad_b = [0] * L\n",
    "    \n",
    "    for i in range(0, L):\n",
    "        scaled_grad = bp_grad[i] * acts[i](z[i], y[i], diff=True)\n",
    "\n",
    "        bp_grad_w[i] = np.dot(x[i].T, scaled_grad)\n",
    "        \n",
    "        relevant_ones = np.ones((1, x[i].shape[0]))\n",
    "        bp_grad_b[i] = np.dot(relevant_ones, scaled_grad)\n",
    "        \n",
    "    return bp_grad_w, bp_grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update it with one line of code (not counting passing `masks` as a parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop_dropout(weights, biases, acts, cost_function,\n",
    "                           train_X, train_Y,\n",
    "                           x, y, z, masks):\n",
    "    L = len(weights) # number of layers\n",
    "    \n",
    "    cost_diff = cost_function(y[-1], train_Y, diff=True)\n",
    "    \n",
    "    # Gradient of cost at each level\n",
    "    bp_grad = [0] * L\n",
    "    \n",
    "    # The last level is special\n",
    "    bp_grad[L-1] = cost_diff * acts[L-1](z[L-1], y[L-1], diff=True)\n",
    "    \n",
    "    # The rest of the levels are just gotten by propagating backward\n",
    "    for i in range(L-2, -1, -1):\n",
    "        scaled_grad = bp_grad[i+1] * acts[i+1](z[i+1], y[i+1], diff=True)\n",
    "        bp_grad[i] = np.dot(scaled_grad, weights[i+1].T)\n",
    "    \n",
    "    # Now adjust for the weights and biases themselves\n",
    "    bp_grad_w = [0] * L\n",
    "    bp_grad_b = [0] * L\n",
    "    \n",
    "    for i in range(0, L):\n",
    "        if i < L-1:\n",
    "            scaled_grad = bp_grad[i] * acts[i](z[i], y[i], diff=True) * masks[i+1]\n",
    "        else:\n",
    "            scaled_grad = bp_grad[i] * acts[i](z[i], y[i], diff=True)\n",
    "\n",
    "        bp_grad_w[i] = np.dot(x[i].T, scaled_grad)\n",
    "        \n",
    "        relevant_ones = np.ones((1, x[i].shape[0]))\n",
    "        bp_grad_b[i] = np.dot(relevant_ones, scaled_grad)\n",
    "        \n",
    "    return bp_grad_w, bp_grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original line `24`, we scale the contributions of the gradient to fit the activiation functions.  Now we also zero it out if that neuron's output was missing; note that we don't dropout from the output, so we need a conditional there.  This change gets propagated across the entire neuron as appropriate in the original lines `26` and `29` (now lines `29` and `32`), as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing it Out\n",
    "\n",
    "In the original paper, they suggest a 20% dropout rate for input units and a 50% dropout rate for hidden units.  They also suggest scaling up the number of hidden units proportionally to compensate for the dropout, as well as dramatically increasing the learning rate and momentum.  Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist_import import get_mnist_nice\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = get_mnist_nice()\n",
    "\n",
    "n = train_X.shape[1]\n",
    "k = train_Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our \"optimize\" code, which just means spawn a neural network and train it up with a mass of hyperparameters. Note that we now use dropout as an additional hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(act_fn, cost_fn, init_fn, learning_rate,\n",
    "             train_X, train_Y,\n",
    "             neuron_sizes, num_epochs, batch_size,\n",
    "             l1_cost=0, l2_cost=0, dropout_rates=None,\n",
    "             momentum=0\n",
    "            ):\n",
    "    np.random.seed(313) # for determinism\n",
    "    \n",
    "    # Step 2: initialize\n",
    "    weights, biases = init_fn(n, k, neuron_sizes)\n",
    "    acts = [act_fn for _ in range(0, len(weights))]\n",
    "    acts[-1] = act_sigmoid # last one is always sigmoid\n",
    "    \n",
    "    if dropout_rates is None: # None means no dropout\n",
    "        dropout_rates = [0 for _ in range(0, len(weights))]\n",
    "    \n",
    "    # Step 3: train\n",
    "    t1 = time.time()\n",
    "    \n",
    "    weight_velocities = [0 for _ in range(0, len(weights))]\n",
    "    biases_velocities = [0 for _ in range(0, len(biases))]\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        # we'll keep track of the cost as we go\n",
    "        total_cost = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for X_mb, Y_mb in get_mini_batches(batch_size, train_X, train_Y):\n",
    "            x, z, y, masks = forward_prop_dropout(weights, biases, acts, X_mb, dropout_rates)\n",
    "\n",
    "            bp_grad_w, bp_grad_b = back_prop_dropout(weights, biases, acts, cost_fn, X_mb, Y_mb, x, y, z, masks)\n",
    "            l1_grad_w = lasso_cost(l1_cost, weights, biases, diff=True)\n",
    "            l2_grad_w = ridge_cost(l2_cost, weights, biases, diff=True)\n",
    "\n",
    "            for i in range(0, len(weights)):\n",
    "                weight_grad = bp_grad_w[i] / len(X_mb)\n",
    "                weight_grad += l1_grad_w[i]\n",
    "                weight_grad += l2_grad_w[i]\n",
    "                \n",
    "                weight_velocities[i] = weight_velocities[i] * momentum + weight_grad\n",
    "                weights[i] -= weight_velocities[i] * learning_rate\n",
    "                \n",
    "                biases_grad = bp_grad_b[i] / len(X_mb)\n",
    "                \n",
    "                biases_velocities[i] = biases_velocities[i] * momentum + biases_grad\n",
    "                biases[i] -= biases_velocities[i] * learning_rate\n",
    "\n",
    "            total_cost += cost_fn(y[-1], Y_mb, aggregate=True)\n",
    "            num_batches += 1\n",
    "\n",
    "        cost = total_cost / num_batches # average cost\n",
    "        print(\"After {0} epochs, have average cost {1:0.7f}. Took {2:0.3f} seconds so far.\".format(epoch, cost, time.time()-t1))\n",
    "        \n",
    "        av = np.mean([np.mean(sum(w*w)) for w in weights])\n",
    "        print(\"Average norm of each weight is {0:0.3f}\".format(av))\n",
    "        \n",
    "        _, _, y = forward_prop(weights, biases, acts, train_X)\n",
    "        train_Y_hat = y[-1]\n",
    "\n",
    "        train_success = classification_success_rate(train_Y_hat, train_Y)\n",
    "        print(\"Got {0:0.3f}% success rate on the training data.\".format(100 * train_success))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return weights, biases, acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's train it with the dropout methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epochs, have average cost 2.0732929. Took 83.812 seconds so far.\n",
      "Average norm of each weight is 5.609\n",
      "Got 89.262% success rate on the training data.\n",
      "\n",
      "After 1 epochs, have average cost 1.1953199. Took 176.944 seconds so far.\n",
      "Average norm of each weight is 6.301\n",
      "Got 90.223% success rate on the training data.\n",
      "\n",
      "After 2 epochs, have average cost 1.1217761. Took 269.073 seconds so far.\n",
      "Average norm of each weight is 6.719\n",
      "Got 90.103% success rate on the training data.\n",
      "\n",
      "After 3 epochs, have average cost 1.0835093. Took 361.983 seconds so far.\n",
      "Average norm of each weight is 6.970\n",
      "Got 91.977% success rate on the training data.\n",
      "\n",
      "After 4 epochs, have average cost 1.0181313. Took 454.245 seconds so far.\n",
      "Average norm of each weight is 6.830\n",
      "Got 92.327% success rate on the training data.\n",
      "\n",
      "After 5 epochs, have average cost 0.9545442. Took 546.972 seconds so far.\n",
      "Average norm of each weight is 6.661\n",
      "Got 93.007% success rate on the training data.\n",
      "\n",
      "After 6 epochs, have average cost 0.9102258. Took 639.422 seconds so far.\n",
      "Average norm of each weight is 6.537\n",
      "Got 93.000% success rate on the training data.\n",
      "\n",
      "After 7 epochs, have average cost 0.9042826. Took 732.310 seconds so far.\n",
      "Average norm of each weight is 6.532\n",
      "Got 93.347% success rate on the training data.\n",
      "\n",
      "After 8 epochs, have average cost 0.8616384. Took 824.218 seconds so far.\n",
      "Average norm of each weight is 6.385\n",
      "Got 93.482% success rate on the training data.\n",
      "\n",
      "After 9 epochs, have average cost 0.8341932. Took 916.591 seconds so far.\n",
      "Average norm of each weight is 6.221\n",
      "Got 93.147% success rate on the training data.\n",
      "\n",
      "After 10 epochs, have average cost 0.8301846. Took 1009.386 seconds so far.\n",
      "Average norm of each weight is 6.126\n",
      "Got 93.950% success rate on the training data.\n",
      "\n",
      "After 11 epochs, have average cost 0.8122024. Took 1101.608 seconds so far.\n",
      "Average norm of each weight is 6.160\n",
      "Got 93.672% success rate on the training data.\n",
      "\n",
      "After 12 epochs, have average cost 0.7893500. Took 1193.888 seconds so far.\n",
      "Average norm of each weight is 6.070\n",
      "Got 93.880% success rate on the training data.\n",
      "\n",
      "After 13 epochs, have average cost 0.7816821. Took 1286.421 seconds so far.\n",
      "Average norm of each weight is 5.942\n",
      "Got 93.948% success rate on the training data.\n",
      "\n",
      "After 14 epochs, have average cost 0.7873239. Took 1379.126 seconds so far.\n",
      "Average norm of each weight is 6.008\n",
      "Got 94.240% success rate on the training data.\n",
      "\n",
      "After 15 epochs, have average cost 0.7838281. Took 1471.650 seconds so far.\n",
      "Average norm of each weight is 5.916\n",
      "Got 93.830% success rate on the training data.\n",
      "\n",
      "After 16 epochs, have average cost 0.7579013. Took 1564.803 seconds so far.\n",
      "Average norm of each weight is 5.828\n",
      "Got 94.058% success rate on the training data.\n",
      "\n",
      "After 17 epochs, have average cost 0.7595237. Took 1657.412 seconds so far.\n",
      "Average norm of each weight is 5.744\n",
      "Got 94.192% success rate on the training data.\n",
      "\n",
      "After 18 epochs, have average cost 0.7578285. Took 1751.595 seconds so far.\n",
      "Average norm of each weight is 5.788\n",
      "Got 94.173% success rate on the training data.\n",
      "\n",
      "After 19 epochs, have average cost 0.7481136. Took 1845.490 seconds so far.\n",
      "Average norm of each weight is 5.684\n",
      "Got 93.432% success rate on the training data.\n",
      "\n",
      "After 20 epochs, have average cost 0.7225736. Took 1939.665 seconds so far.\n",
      "Average norm of each weight is 5.669\n",
      "Got 94.168% success rate on the training data.\n",
      "\n",
      "After 21 epochs, have average cost 0.7265958. Took 2034.237 seconds so far.\n",
      "Average norm of each weight is 5.585\n",
      "Got 94.110% success rate on the training data.\n",
      "\n",
      "After 22 epochs, have average cost 0.7243299. Took 2128.386 seconds so far.\n",
      "Average norm of each weight is 5.587\n",
      "Got 94.377% success rate on the training data.\n",
      "\n",
      "After 23 epochs, have average cost 0.7295931. Took 2222.263 seconds so far.\n",
      "Average norm of each weight is 5.564\n",
      "Got 94.272% success rate on the training data.\n",
      "\n",
      "After 24 epochs, have average cost 0.7169024. Took 2316.780 seconds so far.\n",
      "Average norm of each weight is 5.409\n",
      "Got 94.662% success rate on the training data.\n",
      "\n",
      "After 25 epochs, have average cost 0.7107195. Took 2411.242 seconds so far.\n",
      "Average norm of each weight is 5.404\n",
      "Got 94.600% success rate on the training data.\n",
      "\n",
      "After 26 epochs, have average cost 0.7129848. Took 2505.697 seconds so far.\n",
      "Average norm of each weight is 5.381\n",
      "Got 94.270% success rate on the training data.\n",
      "\n",
      "After 27 epochs, have average cost 0.7015252. Took 2600.901 seconds so far.\n",
      "Average norm of each weight is 5.288\n",
      "Got 94.408% success rate on the training data.\n",
      "\n",
      "After 28 epochs, have average cost 0.6904346. Took 2693.095 seconds so far.\n",
      "Average norm of each weight is 5.266\n",
      "Got 94.627% success rate on the training data.\n",
      "\n",
      "After 29 epochs, have average cost 0.6986432. Took 2785.495 seconds so far.\n",
      "Average norm of each weight is 5.272\n",
      "Got 94.387% success rate on the training data.\n",
      "\n",
      "After 30 epochs, have average cost 0.7017158. Took 2877.920 seconds so far.\n",
      "Average norm of each weight is 5.309\n",
      "Got 94.468% success rate on the training data.\n",
      "\n",
      "After 31 epochs, have average cost 0.6979166. Took 2970.675 seconds so far.\n",
      "Average norm of each weight is 5.166\n",
      "Got 94.763% success rate on the training data.\n",
      "\n",
      "After 32 epochs, have average cost 0.6886676. Took 3062.886 seconds so far.\n",
      "Average norm of each weight is 5.109\n",
      "Got 94.517% success rate on the training data.\n",
      "\n",
      "After 33 epochs, have average cost 0.6845910. Took 3155.535 seconds so far.\n",
      "Average norm of each weight is 5.141\n",
      "Got 94.752% success rate on the training data.\n",
      "\n",
      "After 34 epochs, have average cost 0.6875792. Took 3247.563 seconds so far.\n",
      "Average norm of each weight is 5.148\n",
      "Got 94.755% success rate on the training data.\n",
      "\n",
      "After 35 epochs, have average cost 0.6877664. Took 3339.993 seconds so far.\n",
      "Average norm of each weight is 5.064\n",
      "Got 94.645% success rate on the training data.\n",
      "\n",
      "After 36 epochs, have average cost 0.6805378. Took 3432.294 seconds so far.\n",
      "Average norm of each weight is 5.043\n",
      "Got 94.805% success rate on the training data.\n",
      "\n",
      "After 37 epochs, have average cost 0.6808459. Took 3524.814 seconds so far.\n",
      "Average norm of each weight is 5.021\n",
      "Got 94.642% success rate on the training data.\n",
      "\n",
      "After 38 epochs, have average cost 0.6775456. Took 3617.569 seconds so far.\n",
      "Average norm of each weight is 4.936\n",
      "Got 94.777% success rate on the training data.\n",
      "\n",
      "After 39 epochs, have average cost 0.6807125. Took 3709.440 seconds so far.\n",
      "Average norm of each weight is 4.983\n",
      "Got 94.340% success rate on the training data.\n",
      "\n",
      "Got 94.340% success rate on the training data.\n",
      "Got 94.120% success rate on the test data.\n"
     ]
    }
   ],
   "source": [
    "from basic_nn import *\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "act_fn = act_tanh\n",
    "cost_fn = cost_CE\n",
    "init_fn = initialize_xavier_tanh\n",
    "\n",
    "learning_rate = 2\n",
    "\n",
    "neuron_sizes = [1024]\n",
    "num_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "l1_cost = 0.5\n",
    "l2_cost = 0.25\n",
    "\n",
    "momentum = 0.95\n",
    "\n",
    "dropout_rates = [0.2, 0.5]\n",
    "\n",
    "# Fix up the parameters, as this agrees with what happened in the experiment\n",
    "l1_cost /= len(train_X)\n",
    "l2_cost /= len(train_X)\n",
    "learning_rate *= 1 - momentum\n",
    "\n",
    "# train it up; note this is the whole dataset now\n",
    "weights, biases, acts = optimize(act_fn, cost_fn, init_fn, learning_rate,\n",
    "                                 train_X, train_Y,\n",
    "                                 neuron_sizes, num_epochs, batch_size,\n",
    "                                 l1_cost=l1_cost, l2_cost=l2_cost, dropout_rates=dropout_rates,\n",
    "                                 momentum=momentum\n",
    "                                )\n",
    "\n",
    "_, _, y = forward_prop(weights, biases, acts, train_X)\n",
    "train_Y_hat = y[-1]\n",
    "\n",
    "train_success = classification_success_rate(train_Y_hat, train_Y)\n",
    "print(\"Got {0:0.3f}% success rate on the training data.\".format(100 * train_success))\n",
    "\n",
    "_, _, y = forward_prop(weights, biases, acts, test_X)\n",
    "test_Y_hat = y[-1]\n",
    "\n",
    "test_success = classification_success_rate(test_Y_hat, test_Y)\n",
    "print(\"Got {0:0.3f}% success rate on the test data.\".format(100 * test_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each epoch is taking a painfully long time to run; this because of two factors:\n",
    "\n",
    "1. We are doing a lot of extra calculations, including generating random matrices, then using these masks for multiplication at various points.\n",
    "2. Our neural networks are a lot bigger, to compensate for the dropout rate.\n",
    "\n",
    "Also, the average costs are a lot higher.  This makes sense - dropout is a lot of regularization, and the neurons never know what they're going to have to work with.  It's a much harder problem than mere stochastic gradient descent, with a lot more noise, but it's working.  Slowly.\n",
    "\n",
    "Finally, we notice that overfitting has been completely defeated, but honestly, so has fitting well.  In the original paper, they described using very large learning rates (10-100 times what was originally useful, which for was was `0.5`) and very large momentum (`0.95` to `0.99`), then described another kind of regularization that complemented these things well, called **max-norm** regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization\n",
    "\n",
    "The idea is, for each neuron **n**, we insist that the L2-norm is below some constant *c*.  Recall the L2-norm is the square root of the sum of the squares of the elements.  So the regularization here, rather than integrating it with gradient descent, is a simple scaling of the weights.  The net effect is that the learning rate and momentum sling the weights around violently, then the max-norm regularization sticks them to the surface of the appropriate sphere, so that really we're only changing the angle (but again, sometimes violently).\n",
    "\n",
    "Let's try it out.  To force a vector's norm to be exactly `c`, we can multiply by `c / norm`, which will be less than one if and only if `norm > c`. So, to force a vector's norm to be at most `c`, we can multiply it by `min(1, c / norm)`, which does nothing if `norm <= c` and scales it down to `c` if `norm > c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_scale(mat, bound):\n",
    "    L2 = sum(mat*mat)**0.5 # compute L2 norms of each column; this is now a row vector\n",
    "    scale = np.minimum(np.ones(L2.shape), bound/L2)\n",
    "    return mat*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(act_fn, cost_fn, init_fn, learning_rate,\n",
    "             train_X, train_Y,\n",
    "             neuron_sizes, num_epochs, batch_size,\n",
    "             l1_cost=0, l2_cost=0, dropout_rates=None,\n",
    "             momentum=0, max_norm=float(\"inf\")\n",
    "            ):\n",
    "    np.random.seed(313) # for determinism\n",
    "    \n",
    "    # Step 2: initialize\n",
    "    weights, biases = init_fn(n, k, neuron_sizes)\n",
    "    acts = [act_fn for _ in range(0, len(weights))]\n",
    "    acts[-1] = act_sigmoid # last one is always sigmoid\n",
    "    \n",
    "    if dropout_rates is None: # None means no dropout\n",
    "        dropout_rates = [0 for _ in range(0, len(weights))]\n",
    "    \n",
    "    # Step 3: train\n",
    "    t1 = time.time()\n",
    "    \n",
    "    weight_velocities = [0 for _ in range(0, len(weights))]\n",
    "    biases_velocities = [0 for _ in range(0, len(biases))]\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        # we'll keep track of the cost as we go\n",
    "        total_cost = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for X_mb, Y_mb in get_mini_batches(batch_size, train_X, train_Y):\n",
    "            x, z, y, masks = forward_prop_dropout(weights, biases, acts, X_mb, dropout_rates)\n",
    "\n",
    "            bp_grad_w, bp_grad_b = back_prop_dropout(weights, biases, acts, cost_fn, X_mb, Y_mb, x, y, z, masks)\n",
    "            l1_grad_w = lasso_cost(l1_cost, weights, biases, diff=True)\n",
    "            l2_grad_w = ridge_cost(l2_cost, weights, biases, diff=True)\n",
    "\n",
    "            for i in range(0, len(weights)):\n",
    "                weight_grad = bp_grad_w[i] / len(X_mb)\n",
    "                weight_grad += l1_grad_w[i]\n",
    "                weight_grad += l2_grad_w[i]\n",
    "                \n",
    "                weight_velocities[i] = weight_velocities[i] * momentum + weight_grad\n",
    "                weights[i] -= weight_velocities[i] * learning_rate\n",
    "                weights[i] = max_norm_scale(weights[i], max_norm)\n",
    "                \n",
    "                biases_grad = bp_grad_b[i] / len(X_mb)\n",
    "                \n",
    "                biases_velocities[i] = biases_velocities[i] * momentum + biases_grad\n",
    "                biases[i] -= biases_velocities[i] * learning_rate\n",
    "\n",
    "            total_cost += cost_fn(y[-1], Y_mb, aggregate=True)\n",
    "            num_batches += 1\n",
    "\n",
    "        cost = total_cost / num_batches # average cost\n",
    "        print(\"After {0} epochs, have average cost {1:0.7f}. Took {2:0.3f} seconds so far.\".format(epoch+1, cost, time.time()-t1))\n",
    "        \n",
    "        av = np.mean([np.mean(sum(w*w)**0.5) for w in weights])\n",
    "        print(\"Average norm of each weight is {0:0.3f}\".format(av))\n",
    "        \n",
    "        _, _, y = forward_prop(weights, biases, acts, train_X)\n",
    "        train_Y_hat = y[-1]\n",
    "\n",
    "        train_success = classification_success_rate(train_Y_hat, train_Y)\n",
    "        print(\"Got {0:0.3f}% success rate on the training data.\".format(100 * train_success))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return weights, biases, acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only change required is to insert the max_norm scaling into line `42` of the above. Note that we do not scale the bias (there is no need).  Let's see how this works out.  In the below we use much larger learning rate and momentum, and reduce the L2 penalty to zero, since max-norm regularization does essentially the same job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 epochs, have average cost 2.8655089. Took 108.901 seconds so far.\n",
      "Average norm of each weight is 1.868\n",
      "Got 81.245% success rate on the training data.\n",
      "\n",
      "After 2 epochs, have average cost 1.6802186. Took 231.707 seconds so far.\n",
      "Average norm of each weight is 1.939\n",
      "Got 89.707% success rate on the training data.\n",
      "\n",
      "After 3 epochs, have average cost 1.1510421. Took 352.698 seconds so far.\n",
      "Average norm of each weight is 1.992\n",
      "Got 90.953% success rate on the training data.\n",
      "\n",
      "After 4 epochs, have average cost 1.0763883. Took 472.746 seconds so far.\n",
      "Average norm of each weight is 2.025\n",
      "Got 91.678% success rate on the training data.\n",
      "\n",
      "After 5 epochs, have average cost 1.0256319. Took 596.486 seconds so far.\n",
      "Average norm of each weight is 2.005\n",
      "Got 92.485% success rate on the training data.\n",
      "\n",
      "After 6 epochs, have average cost 0.9776702. Took 714.953 seconds so far.\n",
      "Average norm of each weight is 2.004\n",
      "Got 92.427% success rate on the training data.\n",
      "\n",
      "After 7 epochs, have average cost 0.9382232. Took 834.534 seconds so far.\n",
      "Average norm of each weight is 2.000\n",
      "Got 93.005% success rate on the training data.\n",
      "\n",
      "After 8 epochs, have average cost 0.9192806. Took 958.341 seconds so far.\n",
      "Average norm of each weight is 1.998\n",
      "Got 93.113% success rate on the training data.\n",
      "\n",
      "After 9 epochs, have average cost 0.8768007. Took 1079.922 seconds so far.\n",
      "Average norm of each weight is 1.976\n",
      "Got 93.415% success rate on the training data.\n",
      "\n",
      "After 10 epochs, have average cost 0.8460437. Took 1202.096 seconds so far.\n",
      "Average norm of each weight is 1.970\n",
      "Got 93.753% success rate on the training data.\n",
      "\n",
      "After 11 epochs, have average cost 0.8392322. Took 1326.381 seconds so far.\n",
      "Average norm of each weight is 1.961\n",
      "Got 93.938% success rate on the training data.\n",
      "\n",
      "After 12 epochs, have average cost 0.8292359. Took 1447.845 seconds so far.\n",
      "Average norm of each weight is 1.950\n",
      "Got 93.610% success rate on the training data.\n",
      "\n",
      "After 13 epochs, have average cost 0.8119716. Took 1568.940 seconds so far.\n",
      "Average norm of each weight is 1.956\n",
      "Got 93.918% success rate on the training data.\n",
      "\n",
      "After 14 epochs, have average cost 0.8024965. Took 1686.855 seconds so far.\n",
      "Average norm of each weight is 1.938\n",
      "Got 94.172% success rate on the training data.\n",
      "\n",
      "After 15 epochs, have average cost 0.7921848. Took 1807.681 seconds so far.\n",
      "Average norm of each weight is 1.937\n",
      "Got 94.142% success rate on the training data.\n",
      "\n",
      "After 16 epochs, have average cost 0.7831345. Took 1932.183 seconds so far.\n",
      "Average norm of each weight is 1.924\n",
      "Got 94.208% success rate on the training data.\n",
      "\n",
      "After 17 epochs, have average cost 0.7612700. Took 2089.569 seconds so far.\n",
      "Average norm of each weight is 1.918\n",
      "Got 94.563% success rate on the training data.\n",
      "\n",
      "After 18 epochs, have average cost 0.7590904. Took 2263.695 seconds so far.\n",
      "Average norm of each weight is 1.911\n",
      "Got 94.548% success rate on the training data.\n",
      "\n",
      "After 19 epochs, have average cost 0.7610492. Took 2410.079 seconds so far.\n",
      "Average norm of each weight is 1.914\n",
      "Got 94.498% success rate on the training data.\n",
      "\n",
      "After 20 epochs, have average cost 0.7421811. Took 2587.790 seconds so far.\n",
      "Average norm of each weight is 1.918\n",
      "Got 94.213% success rate on the training data.\n",
      "\n",
      "After 21 epochs, have average cost 0.7365948. Took 2779.029 seconds so far.\n",
      "Average norm of each weight is 1.907\n",
      "Got 94.798% success rate on the training data.\n",
      "\n",
      "After 22 epochs, have average cost 0.7344487. Took 2894.071 seconds so far.\n",
      "Average norm of each weight is 1.884\n",
      "Got 94.523% success rate on the training data.\n",
      "\n",
      "After 23 epochs, have average cost 0.7282672. Took 3008.049 seconds so far.\n",
      "Average norm of each weight is 1.876\n",
      "Got 94.658% success rate on the training data.\n",
      "\n",
      "After 24 epochs, have average cost 0.7326311. Took 3125.567 seconds so far.\n",
      "Average norm of each weight is 1.875\n",
      "Got 94.447% success rate on the training data.\n",
      "\n",
      "After 25 epochs, have average cost 0.7228109. Took 3241.783 seconds so far.\n",
      "Average norm of each weight is 1.867\n",
      "Got 94.987% success rate on the training data.\n",
      "\n",
      "After 26 epochs, have average cost 0.7159762. Took 3353.932 seconds so far.\n",
      "Average norm of each weight is 1.864\n",
      "Got 94.958% success rate on the training data.\n",
      "\n",
      "After 27 epochs, have average cost 0.7096425. Took 3465.763 seconds so far.\n",
      "Average norm of each weight is 1.859\n",
      "Got 94.597% success rate on the training data.\n",
      "\n",
      "After 28 epochs, have average cost 0.7108215. Took 3577.736 seconds so far.\n",
      "Average norm of each weight is 1.849\n",
      "Got 94.662% success rate on the training data.\n",
      "\n",
      "After 29 epochs, have average cost 0.6955870. Took 3689.434 seconds so far.\n",
      "Average norm of each weight is 1.826\n",
      "Got 94.858% success rate on the training data.\n",
      "\n",
      "After 30 epochs, have average cost 0.6991283. Took 3801.847 seconds so far.\n",
      "Average norm of each weight is 1.819\n",
      "Got 94.842% success rate on the training data.\n",
      "\n",
      "After 31 epochs, have average cost 0.6917457. Took 3919.928 seconds so far.\n",
      "Average norm of each weight is 1.820\n",
      "Got 94.725% success rate on the training data.\n",
      "\n",
      "After 32 epochs, have average cost 0.6919848. Took 4038.969 seconds so far.\n",
      "Average norm of each weight is 1.820\n",
      "Got 94.717% success rate on the training data.\n",
      "\n",
      "After 33 epochs, have average cost 0.6919119. Took 4157.844 seconds so far.\n",
      "Average norm of each weight is 1.814\n",
      "Got 94.975% success rate on the training data.\n",
      "\n",
      "After 34 epochs, have average cost 0.6791455. Took 4281.864 seconds so far.\n",
      "Average norm of each weight is 1.809\n",
      "Got 95.022% success rate on the training data.\n",
      "\n",
      "After 35 epochs, have average cost 0.6899398. Took 4405.965 seconds so far.\n",
      "Average norm of each weight is 1.811\n",
      "Got 94.963% success rate on the training data.\n",
      "\n",
      "After 36 epochs, have average cost 0.6854844. Took 4531.134 seconds so far.\n",
      "Average norm of each weight is 1.806\n",
      "Got 94.737% success rate on the training data.\n",
      "\n",
      "After 37 epochs, have average cost 0.6816771. Took 4648.247 seconds so far.\n",
      "Average norm of each weight is 1.791\n",
      "Got 94.848% success rate on the training data.\n",
      "\n",
      "After 38 epochs, have average cost 0.6806935. Took 4769.122 seconds so far.\n",
      "Average norm of each weight is 1.772\n",
      "Got 94.860% success rate on the training data.\n",
      "\n",
      "After 39 epochs, have average cost 0.6739370. Took 4895.335 seconds so far.\n",
      "Average norm of each weight is 1.780\n",
      "Got 94.710% success rate on the training data.\n",
      "\n",
      "After 40 epochs, have average cost 0.6847465. Took 5016.874 seconds so far.\n",
      "Average norm of each weight is 1.776\n",
      "Got 94.915% success rate on the training data.\n",
      "\n",
      "Got 94.915% success rate on the training data.\n",
      "Got 95.010% success rate on the test data.\n"
     ]
    }
   ],
   "source": [
    "from basic_nn import *\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "act_fn = act_tanh\n",
    "cost_fn = cost_CE\n",
    "init_fn = initialize_xavier_tanh\n",
    "\n",
    "learning_rate = 2\n",
    "\n",
    "neuron_sizes = [1024]\n",
    "num_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "l1_cost = 0.5\n",
    "l2_cost = 0.0\n",
    "\n",
    "momentum = 0.95\n",
    "\n",
    "dropout_rates = [0.2, 0.5]\n",
    "max_norm = 5\n",
    "\n",
    "# Fix up the parameters, as this agrees with what happened in the experiment\n",
    "l1_cost /= len(train_X)\n",
    "l2_cost /= len(train_X)\n",
    "learning_rate *= 1 - momentum\n",
    "\n",
    "# train it up; note this is the whole dataset now\n",
    "weights, biases, acts = optimize(act_fn, cost_fn, init_fn, learning_rate,\n",
    "                                 train_X, train_Y,\n",
    "                                 neuron_sizes, num_epochs, batch_size,\n",
    "                                 l1_cost=l1_cost, l2_cost=l2_cost, dropout_rates=dropout_rates,\n",
    "                                 momentum=momentum, max_norm=max_norm\n",
    "                                )\n",
    "\n",
    "_, _, y = forward_prop(weights, biases, acts, train_X)\n",
    "train_Y_hat = y[-1]\n",
    "\n",
    "train_success = classification_success_rate(train_Y_hat, train_Y)\n",
    "print(\"Got {0:0.3f}% success rate on the training data.\".format(100 * train_success))\n",
    "\n",
    "_, _, y = forward_prop(weights, biases, acts, test_X)\n",
    "test_Y_hat = y[-1]\n",
    "\n",
    "test_success = classification_success_rate(test_Y_hat, test_Y)\n",
    "print(\"Got {0:0.3f}% success rate on the test data.\".format(100 * test_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a slight improvement, getting to 95% test accuracy with no overfitting, but it took a very long time, and it's not sustainable to continue like this.  The general rule is to make your networks overfit first, then regularize the crap out of them, then repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions and Future Work\n",
    "\n",
    "Each time we add more aspects to our training system, we increase the running time dramatically. Moreover, it seems clear by examination that after we introduced dropout, we need a *lot* more epochs - we are not overfitting, but we are moving our costs down very slowly, and we just need a lot more time. For example, in the original paper on Dropout, they used one million weight updates for training, which would be the equivalent of over a thousand epochs as defined above. Unfortunately, I'm on a budget computer, and CPU hours are on a premium (as is my time). We need to make the most of the resources we have.\n",
    "\n",
    "There are things we can do to speed up each epoch (besides move to a more efficient platform like <a href=\"http://torch.ch/\">Torch</a> or <a href=\"https://www.tensorflow.org/versions/r0.8/get_started/index.html\">TensorFlow</a>, which helps dramatically but defeats the purpose of the project, which is to expose and explore the innards) to make our `optimize` method more efficient, such as not computing factors we're not using. Here, for example, we could remove reference to L2 regularization, since we've replaced it with max-norm regularization.\n",
    "\n",
    "However, these are small measures.  Next, we will talk about dimensionality reduction, which shrinks the size of (some of) our matrices, as well as unsupervised pre-training, which will help us start with a better network and not need so many epochs at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
