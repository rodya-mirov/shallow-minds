{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A First Simulation\n",
    "\n",
    "Now, finally, we're going to actually train a neural network!  We'll use the techniques we've learned so far, and compare the results we get with various activation functions and cost functions we've talking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To do this, we need data.  All kinds of data will work, but there are standard benchmark datasets that people use, and we'll use one of them.  This is called **MNIST**, and consists of a large number of handwritten digits (0 to 9) with labels, dividing into training and test sets (60000 and 10000 examples, respectively).\n",
    "\n",
    "You can get the data <a href=\"http://yann.lecun.com/exdb/mnist/\">here</a>, but the format is annoying.  You could read about the format and write your own parser, or you could use the parser <a href=\"http://g.sweyla.com/blog/2012/mnist-numpy/\">here</a>.  To make that even easier for my purposes, I've written a convenience function on top of his parser; you can get it <a href=\"mnist.py\">here</a>, although it requires the files to be in your current working directory to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist_import import get_mnist_nice\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = get_mnist_nice()\n",
    "\n",
    "n = train_X.shape[1]\n",
    "k = train_Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $n=784$ and $k=10$.\n",
    "\n",
    "This is because each data point is a 28x28 greyscale image, and the floats indicate darkness of each pixel, written out into a single long row.  Since there are 50000 training examples and 10000 test examples, `train_X` is a 50000x784 array and `test_X` is a 10000x784 array.\n",
    "\n",
    "We aim to classify the digits.  Since each digit can be 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9, but these don't naturally have any kind of useful ordering, we consider them each their own class.  So the 4th column (for example) of `train_Y` is always 0 or 1, and is 1 if and only if that row is a 3 (we start from zero).  Thus `train_Y` is a 50000x10 array, and `test_Y` is a 10000x10 array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Putting the Pieces Together\n",
    "\n",
    "Now that we have the dataset and the problem picked out, we need to do the following:\n",
    "1. Decide on the architecture of the neural network, including hyperparameters, cost functions, etc.\n",
    "2. Initialize the neural network\n",
    "3. Train the neural network\n",
    "4. Evaluate the neural network\n",
    "\n",
    "We aren't too worried about accuracy right now, we just want to kick the tires and make sure everything is working.  So we'll go with a basic setup, with sigmoid neurons and only one hidden layer.\n",
    "\n",
    "I've placed all the code from the preceding notebooks in the file <a href=\"basic_nn.py\">basic_nn.py</a>, so we can just invoke the old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from basic_nn import *\n",
    "\n",
    "np.random.seed(31)\n",
    "\n",
    "# Step 1: pick architecture (in prose above)\n",
    "cost_function = cost_CE     # this is a classification problem\n",
    "learning_rate = 0.125       # picked arbitrarily, seems to work okay\n",
    "\n",
    "# Step 2: initialize\n",
    "\n",
    "#100 input neurons, 100 neurons in a hidden layer, so 100+100+10 total\n",
    "neuron_sizes = [100, 100]\n",
    "\n",
    "weights, biases = initialize_xavier_sigmoid(n, k, neuron_sizes)\n",
    "acts = [act_sigmoid for _ in range(0, len(weights))] # all sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: I picked the learning rate with a little care.  I started with 1, but the error wasn't consistently dropping.  Then I cut it in half (down to 0.5 now) but the error would again start to rise, after a certain point.  Then I cut it in half (down to 0.25) and got the same problem.  Finally, I got it down to 0.125 and the error consistently dropped with that learning rate.  It's annoying, but this really is the way you pick learning rates; pick a number that causes blowup, then keep halving it until it doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration through the training data is called an *epoch*, probably because it takes forever.  There are two ways to know when to stop -- when the training converges, or when you run a specified number of epochs.  The second choice seems more dominant.  For sake of time, let's do 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 6.9314718 before epoch 0; took 5.159 seconds so far.\n",
      "Cost 5.3662717 before epoch 1; took 10.635 seconds so far.\n",
      "Cost 4.5788508 before epoch 2; took 15.947 seconds so far.\n",
      "Cost 4.1597876 before epoch 3; took 21.177 seconds so far.\n",
      "Cost 3.9122014 before epoch 4; took 26.492 seconds so far.\n",
      "Cost 3.7528381 before epoch 5; took 31.919 seconds so far.\n",
      "Cost 3.6435288 before epoch 6; took 37.667 seconds so far.\n",
      "Cost 3.5649101 before epoch 7; took 43.407 seconds so far.\n",
      "Cost 3.5062749 before epoch 8; took 48.587 seconds so far.\n",
      "Cost 3.4612781 before epoch 9; took 53.992 seconds so far.\n",
      "Cost 3.4259450 before epoch 10; took 59.105 seconds so far.\n",
      "Cost 3.3976713 before epoch 11; took 64.285 seconds so far.\n",
      "Cost 3.3746859 before epoch 12; took 69.413 seconds so far.\n",
      "Cost 3.3557465 before epoch 13; took 74.639 seconds so far.\n",
      "Cost 3.3399587 before epoch 14; took 80.061 seconds so far.\n",
      "Cost 3.3266638 before epoch 15; took 85.277 seconds so far.\n",
      "Cost 3.3153672 before epoch 16; took 90.438 seconds so far.\n",
      "Cost 3.3056914 before epoch 17; took 95.634 seconds so far.\n",
      "Cost 3.2973435 before epoch 18; took 100.779 seconds so far.\n",
      "Cost 3.2900939 before epoch 19; took 105.957 seconds so far.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: train\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "old_predictions = [0] * num_epochs\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    x, z, y = forward_prop(weights, biases, acts, train_X)\n",
    "    \n",
    "    bp_grad_w, bp_grad_b = back_prop(weights, biases, acts, cost_function, train_X, train_Y, x, y, z)\n",
    "    \n",
    "    # Just for fun, let's save the successive predictions as we go\n",
    "    old_predictions[epoch] = y[-1]\n",
    "    \n",
    "    for i in range(0, len(weights)):\n",
    "        weights[i] -= learning_rate * bp_grad_w[i] / len(train_X)\n",
    "        biases[i] -= learning_rate * bp_grad_b[i] / len(train_X)\n",
    "    \n",
    "    cost = cost_function(y[-1], train_Y)\n",
    "    print(\"Cost {2:0.7f} before epoch {0}; took {1:0.3f} seconds so far.\".format(epoch, time.time()-t1, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some takeaways:\n",
    "1. Convergence is happening, but very slowly.  We need a lot more epochs.\n",
    "2. Epochs take a very long time (5 seconds apiece on my laptop).\n",
    "\n",
    "Points (1) and (2) are certainly in conflict, and in the next notebook we'll discuss stochastic gradient descent, which will let us accomplish more in each epoch by subdividing the data into more manageable pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Results\n",
    "\n",
    "Now, let's look at classification error.  That is, how often is our network getting the right answers?  The output of the network (for each row) is whatever class has the highest score.  We can easily get this from the `np.argmax(arr, axis=0)` function, then just check how often the prediction matches the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_success_rate(y_hat, y):\n",
    "    predicted_classes = np.argmax(y_hat, axis=1)\n",
    "    actual_classes = np.argmax(y, axis=1)\n",
    "    errors = predicted_classes - actual_classes\n",
    "    \n",
    "    return 1 - (np.count_nonzero(errors) / len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate before epoch 0: 9.872%\n",
      "Success rate before epoch 1: 11.237%\n",
      "Success rate before epoch 2: 11.237%\n",
      "Success rate before epoch 3: 11.237%\n",
      "Success rate before epoch 4: 11.237%\n",
      "Success rate before epoch 5: 11.237%\n",
      "Success rate before epoch 6: 11.238%\n",
      "Success rate before epoch 7: 11.238%\n",
      "Success rate before epoch 8: 11.238%\n",
      "Success rate before epoch 9: 11.238%\n",
      "Success rate before epoch 10: 11.238%\n",
      "Success rate before epoch 11: 11.238%\n",
      "Success rate before epoch 12: 11.238%\n",
      "Success rate before epoch 13: 11.238%\n",
      "Success rate before epoch 14: 11.238%\n",
      "Success rate before epoch 15: 11.238%\n",
      "Success rate before epoch 16: 11.238%\n",
      "Success rate before epoch 17: 11.238%\n",
      "Success rate before epoch 18: 11.238%\n",
      "Success rate before epoch 19: 11.238%\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_epochs):\n",
    "    success = classification_success_rate(old_predictions[i], train_Y)\n",
    "    print(\"Success rate before epoch {0}: {1:0.3f}%\".format(i, 100*success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success rate is not inspiring, I admit.  The honest truth is that with thousands or even millions of epochs, this would improve dramatically, but I don't have time for that and neither does anyone else.  There is a better way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
